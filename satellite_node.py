import socket
import torch
import pickle
import time
import threading
from enum import Enum
from utils.inference_utils import get_dnn_model, model_partition
from predictor import predictor_utils
from utils import dataset_utils

class SatelliteType(Enum):
    REMOTE_SENSING = "remote_sensing"  # é¥æ„Ÿå«æ˜Ÿ
    LEO_COMPUTING = "leo_computing"  # è®¡ç®—å«æ˜Ÿ


class SatelliteNode:
    def __init__(self, node_id: str, satellite_type: SatelliteType,
                 ip: str, port: int, compute_capacity: float, device: str = "cpu"):
        self.node_id = node_id                      #å«æ˜Ÿç¼–å·
        self.satellite_type = satellite_type        #å«æ˜Ÿç§ç±» é¥æ„Ÿorè®¡ç®—
        self.ip = ip                                #ipå·ï¼Œç”¨äºæ¨¡æ‹Ÿä¸åŒä¸»æœºé—´é€šä¿¡
        self.port = port                            #ç«¯å£å·ï¼Œæ¨¡æ‹Ÿä¸»æœºä¸åŒçš„åº”ç”¨ç¨‹åº
        self.compute_capacity = compute_capacity    #å«æ˜ŸèŠ‚ç‚¹ç®—åŠ›
        self.device = device                        #èŠ‚ç‚¹ä½¿ç”¨cpu/cuda

        # ç½‘ç»œä¿¡æ¯
        self.neighbor_nodes = {}                    # å­˜å‚¨å‘ç°çš„é‚»å±…

        # æ·»åŠ æ¨¡å‹å’Œé¢„æµ‹å™¨
        self.current_model = None                   #ï¼Ÿï¼Ÿ
        self.assigned_layers = None                 #ï¼Ÿï¼Ÿ
        self.predictor_dict = {}                    # æ—¶å»¶é¢„æµ‹å™¨å­—å…¸

        # ä»»åŠ¡çŠ¶æ€
        self.current_task = None
        self.is_busy = False

        self.model_type = None
        self.input_data = None
        self.max_latency = None

        # æ·»åŠ åœ°é¢ç«™ä¿¡æ¯
        self.ground_stations = {}

        print(f"åˆ›å»ºå«æ˜ŸèŠ‚ç‚¹: {self.node_id} ({self.satellite_type.value})")

    def add_ground_station(self, station_id: str, station_info: dict):
        """æ·»åŠ åœ°é¢ç«™ä¿¡æ¯"""
        self.ground_stations[station_id] = station_info
        print(f"{self.node_id} æ·»åŠ åœ°é¢ç«™: {station_id}")

    def get_info(self):
        """è·å–å«æ˜Ÿå®Œæ•´ä¿¡æ¯"""
        return {
            'node_id': self.node_id,
            'type': self.satellite_type.value,
            'ip': self.ip,
            'port': self.port,
            'compute_capacity': self.compute_capacity,
            'device': self.device
        }

    def start_discovery_service(self):
        """å¯åŠ¨å‘ç°æœåŠ¡"""

        def discovery_handler():
            try:
                server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                server_socket.bind((self.ip, self.port))
                server_socket.listen(5)

                print(f"{self.node_id} å‘ç°æœåŠ¡å¯åŠ¨åœ¨ {self.ip}:{self.port}")

                while True:
                    conn, addr = server_socket.accept()
                    threading.Thread(
                        target=self._handle_neighbor_connection,
                        args=(conn, addr)
                    ).start()

            except Exception as e:
                print(f"å‘ç°æœåŠ¡é”™è¯¯: {e}")

        thread = threading.Thread(target=discovery_handler, daemon=True)
        thread.start()
        return thread

    def _handle_neighbor_connection(self, conn, addr):
        """å¤„ç†æ‰€æœ‰è¿æ¥è¯·æ±‚ï¼ˆé‚»å±…å‘ç° + ä»»åŠ¡æäº¤ï¼‰"""
        try:
            # æ¥æ”¶å®Œæ•´æ•°æ®
            data = b""
            conn.settimeout(60.0)  # ğŸ¯ å¢å¤§è¶…æ—¶æ—¶é—´

            while True:
                chunk = conn.recv(65536)  # ğŸ¯ å¢å¤§åˆ°64KBæ¯æ¬¡
                if not chunk:
                    break
                data += chunk
                # å°è¯•è§£ææ•°æ®ï¼Œå¦‚æœæˆåŠŸåˆ™åœæ­¢æ¥æ”¶
                try:
                    message = pickle.loads(data)
                    break  # æˆåŠŸè§£æï¼Œåœæ­¢æ¥æ”¶
                except:
                    continue  # æ•°æ®ä¸å®Œæ•´ï¼Œç»§ç»­æ¥æ”¶

            if not data:
                return

            try:
                message = pickle.loads(data)

                # åˆ¤æ–­æ¶ˆæ¯ç±»å‹ï¼šå¦‚æœæ˜¯ä»»åŠ¡æ¶ˆæ¯
                if isinstance(message, dict) and 'task_id' in message:
                    print(f"{self.node_id} æ¥æ”¶åˆ°ä»»åŠ¡: {message['task_id']}")

                    if self.satellite_type == SatelliteType.REMOTE_SENSING and not self.is_busy:
                        result = self.assign_task(message)
                    else:
                        result = {"status": "rejected", "message": "èŠ‚ç‚¹ç¹å¿™æˆ–éåè°ƒèŠ‚ç‚¹"}

                    # å‘é€å“åº”
                    response_data = pickle.dumps(result)
                    conn.sendall(response_data)

                # åˆ¤æ–­æ¶ˆæ¯ç±»å‹ï¼šå¦‚æœæ˜¯é‚»å±…å‘ç°æ¶ˆæ¯
                elif isinstance(message, dict) and 'node_id' in message and 'type' in message:
                    print(f"{self.node_id} å‘ç°é‚»å±…: {message['node_id']}")

                    self.neighbor_nodes[message['node_id']] = {
                        'node_id': message['node_id'],
                        'type': message['type'],
                        'ip': message['ip'],
                        'port': message['port'],
                        'compute_capacity': message['compute_capacity'],
                        'device': message['device']
                    }

                    ack = self.get_info()
                    ack['status'] = 'ack'
                    conn.sendall(pickle.dumps(ack))

                else:
                    conn.sendall(pickle.dumps({"status": "error", "message": "æœªçŸ¥æ¶ˆæ¯æ ¼å¼"}))

            except Exception as e:
                print(f"è§£ææ¶ˆæ¯é”™è¯¯: {e}")
                conn.sendall(pickle.dumps({"status": "error", "message": f"è§£æé”™è¯¯: {str(e)}"}))

        except socket.timeout:
            print("æ¥æ”¶æ•°æ®è¶…æ—¶")
        except Exception as e:
            print(f"å¤„ç†è¿æ¥æ¶ˆæ¯é”™è¯¯: {e}")
        finally:
            conn.close()

    def discover_neighbor(self, neighbor_ip: str, neighbor_port: int):
        """ä¸»åŠ¨å‘ç°ä¸€ä¸ªé‚»å±…"""
        try:
            conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            conn.connect((neighbor_ip, neighbor_port))

            # å‘é€æœ¬èŠ‚ç‚¹å®Œæ•´ä¿¡æ¯
            hello_msg = self.get_info()
            conn.send(pickle.dumps(hello_msg))

            # ç­‰å¾…å›å¤ï¼ˆç°åœ¨åŒ…å«å¯¹æ–¹å®Œæ•´ä¿¡æ¯ï¼‰
            response_data = conn.recv(1024)
            response = pickle.loads(response_data)

            if response['status'] == 'ack':
                print(f"{self.node_id} æˆåŠŸè¿æ¥åˆ° {response['node_id']}")

                # ä¿®å¤ï¼šä¿å­˜å®Œæ•´çš„é‚»å±…ä¿¡æ¯ï¼ˆä»å›å¤ä¸­è·å–ï¼‰
                self.neighbor_nodes[response['node_id']] = {
                    'node_id': response['node_id'],
                    'type': response['type'],
                    'ip': response['ip'],
                    'port': response['port'],
                    'compute_capacity': response['compute_capacity'],
                    'device': response['device']
                }

                return True
            else:
                print(f"{self.node_id} è¿æ¥è¢«æ‹’ç»")
                return False

        except Exception as e:
            print(f"{self.node_id} è¿æ¥å¤±è´¥: {e}")
            return False
        finally:
            conn.close()

    def get_network_status(self):
        """è·å–ç½‘ç»œçŠ¶æ€ä¿¡æ¯"""
        return {
            'node_id': self.node_id,
            'neighbors_count': len(self.neighbor_nodes),
            'neighbors': list(self.neighbor_nodes.keys()),
            'compute_capacity': self.compute_capacity,
            'device': self.device
        }

    def print_network_info(self):
        """æ‰“å°ç½‘ç»œæ‹“æ‰‘ä¿¡æ¯"""
        print(f"\n{self.node_id} çš„ç½‘ç»œæ‹“æ‰‘:")
        print(f"æœ¬èŠ‚ç‚¹: {self.satellite_type.value}, ç®—åŠ›: {self.compute_capacity}, è®¾å¤‡: {self.device}")
        print(f"é‚»å±…èŠ‚ç‚¹ ({len(self.neighbor_nodes)} ä¸ª):")

        for neighbor_id, info in self.neighbor_nodes.items():
            print(f"    {neighbor_id}: {info['type']}, ç®—åŠ›: {info['compute_capacity']}, è®¾å¤‡: {info['device']}")

    def assign_task(self, task_spec):
        """æ¥æ”¶å¹¶åˆ†é…æ¨ç†ä»»åŠ¡"""
        if self.is_busy:
            return {"status": "busy", "message": "èŠ‚ç‚¹æ­£å¿™"}

        print(f"{self.node_id} æ¥æ”¶åˆ°ä»»åŠ¡: {task_spec['task_id']}")
        self.is_busy = True
        self.current_task = task_spec

        # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
        self.model_type = task_spec['model_type']
        self.input_data = task_spec['input_data']
        self.max_latency = task_spec['max_latency']

        # åŠ è½½æ¨¡å‹
        self.current_model = get_dnn_model(self.model_type)

        # å¦‚æœæ˜¯é¥æ„Ÿå«æ˜Ÿï¼Œå¯åŠ¨åè°ƒæµç¨‹
        if self.satellite_type == SatelliteType.REMOTE_SENSING:
            result = self.coordinate_inference()
        else:
            # å…¶ä»–å«æ˜Ÿç­‰å¾…åˆ†é…
            result = {"status": "waiting_for_assignment"}

        self.is_busy = False
        return result

    def coordinate_inference(self):
        """åè°ƒæ¨ç†è¿‡ç¨‹ï¼ˆé¥æ„Ÿå«æ˜Ÿä¸“ç”¨ï¼‰"""
        print(f"{self.node_id} å¼€å§‹åè°ƒæ¨ç†...")

        # 1. æ”¶é›†ç½‘ç»œæ‹“æ‰‘ä¿¡æ¯
        network_info = self.collect_network_info()

        # 2. ä½¿ç”¨æ™ºèƒ½åˆ†å‰²ç®—æ³•ï¼ˆè€Œä¸æ˜¯ç®€å•å¹³å‡åˆ†é…ï¼‰
        partition_plan = self.calculate_optimal_partition(network_info)

        # 3. åˆ†å‘ä»»åŠ¡åˆ°å„å«æ˜Ÿ
        execution_result = self.distribute_and_execute(partition_plan)

        return execution_result

    def collect_network_info(self):
        """æ”¶é›†ç½‘ç»œæ‹“æ‰‘å’ŒèŠ‚ç‚¹ä¿¡æ¯ï¼ˆåŒ…å«åœ°é¢ç«™ï¼‰"""
        print(f"{self.node_id} æ”¶é›†ç½‘ç»œä¿¡æ¯...")

        network_info = {
            'nodes': {},
            'links': {},
            'ground_stations': self.ground_stations.copy(),
            'timestamp': time.time()
        }

        # æ·»åŠ æœ¬èŠ‚ç‚¹ä¿¡æ¯
        network_info['nodes'][self.node_id] = {
            'node_id': self.node_id,
            'type': self.satellite_type.value,
            'compute_capacity': self.compute_capacity,
            'device': self.device,
            'ip': self.ip,
            'port': self.port
        }

        # æ·»åŠ é‚»å±…èŠ‚ç‚¹ä¿¡æ¯
        for neighbor_id, neighbor_info in self.neighbor_nodes.items():
            network_info['nodes'][neighbor_id] = {
                'node_id': neighbor_id,
                'type': neighbor_info['type'],
                'compute_capacity': neighbor_info['compute_capacity'],
                'device': neighbor_info['device'],
                'ip': neighbor_info['ip'],
                'port': neighbor_info['port']
            }

            # æ·»åŠ é“¾è·¯ä¿¡æ¯ï¼ˆç®€åŒ–ä¼°è®¡ï¼‰
            link_key = f"{self.node_id}->{neighbor_id}"
            network_info['links'][link_key] = {
                'bandwidth': self.estimate_link_bandwidth(neighbor_info),
                'latency': self.estimate_link_latency(neighbor_info)
            }

        # æ·»åŠ åœ°é¢ç«™åˆ°èŠ‚ç‚¹çš„é“¾è·¯
        for station_id, station_info in self.ground_stations.items():
            link_key = f"{self.node_id}->{station_id}"
            network_info['links'][link_key] = {
                'bandwidth': station_info.get('bandwidth', 200.0),  # åœ°é¢ç«™å¸¦å®½æ›´é«˜
                'latency': station_info.get('latency', 50.0)  # å«æ˜Ÿåˆ°åœ°é¢å»¶è¿Ÿ
            }

        print(
            f"æ”¶é›†åˆ° {len(network_info['nodes'])} ä¸ªèŠ‚ç‚¹, {len(network_info['links'])} æ¡é“¾è·¯, {len(network_info['ground_stations'])} ä¸ªåœ°é¢ç«™")
        return network_info

    def estimate_link_bandwidth(self, neighbor_info):
        """æ›´çœŸå®çš„å¸¦å®½ä¼°è®¡"""
        # å¦‚æœæ˜¯è¿æ¥åˆ°åœ°é¢ç«™ï¼Œå¸¦å®½è¾ƒä½ï¼ˆæ˜Ÿåœ°é“¾è·¯ï¼‰
        if neighbor_info.get('type') == 'ground_station' or 'ground' in neighbor_info.get('node_id', '').lower():
            return 50.0  # æ˜Ÿåœ°å¸¦å®½è¾ƒä½
        # æ˜Ÿé—´é“¾è·¯å¸¦å®½è¾ƒé«˜
        elif self.satellite_type == SatelliteType.REMOTE_SENSING:
            return 100.0
        else:
            return 80.0

    def estimate_link_latency(self, neighbor_info):
        """ä¼°è®¡é“¾è·¯å»¶è¿Ÿï¼ˆåŸºäºå«æ˜Ÿç±»å‹ï¼‰"""
        # ç®€åŒ–ä¼°è®¡
        if self.satellite_type == SatelliteType.REMOTE_SENSING:
            return 10.0  # ms
        else:
            return 5.0  # ms

    def calculate_optimal_partition(self, network_info):
        if hasattr(self, 'current_task') and self.current_task.get('test_type') == 'single_satellite':
            print(f"{self.node_id} ä½¿ç”¨å•æ˜Ÿæ¨ç†æ¨¡å¼")
            total_layers = len(self.current_model)

            # é€‰æ‹©ç®—åŠ›æœ€å¼ºçš„èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬åœ°é¢ç«™ï¼‰
            all_nodes = list(network_info['nodes'].keys())
            best_node = max(all_nodes, key=lambda x: network_info['nodes'][x]['compute_capacity'])

            return [{
                'node_id': best_node,
                'layer_range': (0, total_layers),
                'compute_load': total_layers
            }]

        """æ”¹è¿›çš„åˆ†å‰²æ–¹æ¡ˆï¼Œè€ƒè™‘åœ°é¢ç«™é«˜ç®—åŠ›"""
        print(f"{self.node_id} è®¡ç®—åˆ†å‰²æ–¹æ¡ˆ...")

        available_nodes = list(network_info['nodes'].keys())

        # è¯†åˆ«åœ°é¢ç«™å’Œå«æ˜ŸèŠ‚ç‚¹
        ground_stations = []
        satellite_nodes = []

        for node_id in available_nodes:
            node_info = network_info['nodes'][node_id]
            # é€šè¿‡typeå­—æ®µè¯†åˆ«åœ°é¢ç«™
            if node_info.get('type') == 'ground_station':
                ground_stations.append(node_id)
            else:
                satellite_nodes.append(node_id)

        print(f"å¯ç”¨èŠ‚ç‚¹: å«æ˜Ÿ{satellite_nodes}, åœ°é¢ç«™{ground_stations}")

        total_layers = len(self.current_model)
        print(f"æ¨¡å‹æ€»å±‚æ•°: {total_layers}")

        partition_plan = []

        # ä¼˜å…ˆä½¿ç”¨åœ°é¢ç«™çš„é«˜ç®—åŠ›
        if ground_stations:
            ground_station_id = ground_stations[0]
            split_point = int(total_layers * 0.6)  # å‰60%åœ¨å«æ˜Ÿï¼Œå40%åœ¨åœ°é¢ç«™

            # å«æ˜ŸèŠ‚ç‚¹åˆ†é…å‰åŠéƒ¨åˆ†
            if satellite_nodes:
                current_layer = 0
                for i, sat_id in enumerate(satellite_nodes):
                    if current_layer >= split_point:
                        break
                    # ç®€å•å¹³å‡åˆ†é…å‰åŠéƒ¨åˆ†
                    layers_per_sat = (split_point - current_layer) // (len(satellite_nodes) - i)
                    end_layer = min(current_layer + layers_per_sat, split_point)
                    partition_plan.append({
                        'node_id': sat_id,
                        'layer_range': (current_layer, end_layer),
                        'compute_load': end_layer - current_layer
                    })
                    current_layer = end_layer

            # åœ°é¢ç«™åˆ†é…è®¡ç®—å¯†é›†çš„ååŠéƒ¨åˆ†
            partition_plan.append({
                'node_id': ground_station_id,
                'layer_range': (split_point, total_layers),
                'compute_load': total_layers - split_point
            })
        else:
            # å¦‚æœæ²¡æœ‰åœ°é¢ç«™ï¼Œä½¿ç”¨åŸæ¥çš„å«æ˜Ÿåˆ†é…ç­–ç•¥
            total_compute = sum(network_info['nodes'][node]['compute_capacity'] for node in available_nodes)
            current_layer = 0

            for i, node_id in enumerate(available_nodes):
                if current_layer >= total_layers:
                    break

                node_compute = network_info['nodes'][node_id]['compute_capacity']
                compute_ratio = node_compute / total_compute
                layers_to_assign = max(1, int(total_layers * compute_ratio))
                end_layer = min(current_layer + layers_to_assign, total_layers)

                partition_plan.append({
                    'node_id': node_id,
                    'layer_range': (current_layer, end_layer),
                    'compute_load': end_layer - current_layer
                })
                current_layer = end_layer

            # å¦‚æœè¿˜æœ‰å‰©ä½™å±‚æ•°ï¼Œåˆ†é…ç»™æœ€åä¸€ä¸ªèŠ‚ç‚¹
            if current_layer < total_layers and partition_plan:
                last_partition = partition_plan[-1]
                last_partition['layer_range'] = (last_partition['layer_range'][0], total_layers)
                last_partition['compute_load'] = total_layers - last_partition['layer_range'][0]

        print(f"åˆ†å‰²æ–¹æ¡ˆ: {partition_plan}")
        return partition_plan

    def distribute_and_execute(self, partition_plan):
        """åˆ†å‘ä»»åŠ¡å¹¶æ‰§è¡ŒååŒæ¨ç† - ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿è¿”å›æœ€ç»ˆè¾“å‡º"""
        print(f"{self.node_id} åˆ†å‘ä»»åŠ¡åˆ° {len(partition_plan)} ä¸ªèŠ‚ç‚¹...")

        execution_results = {
            'success': False,
            'total_latency': 0.0,
            'partition_plan': partition_plan,
            'node_results': {},
            'final_output': None  # ç¡®ä¿æœ‰è¿™ä¸ªå­—æ®µ
        }

        current_data = self.input_data
        total_latency = 0.0

        try:
            # æ‰§è¡Œå„èŠ‚ç‚¹çš„åˆ†é…ä»»åŠ¡
            for i, partition in enumerate(partition_plan):
                node_id = partition['node_id']
                layer_range = partition['layer_range']

                print(f"  åˆ†åŒº {i + 1}: {node_id} æ‰§è¡Œå±‚ {layer_range}")

                if node_id == self.node_id:
                    # æœ¬èŠ‚ç‚¹æ‰§è¡Œ
                    output_data, exec_time = self.execute_assigned_layers(current_data, layer_range)
                    execution_results['node_results'][node_id] = {
                        'execution_time': exec_time,
                        'status': 'success'
                    }
                    total_latency += exec_time
                    current_data = output_data  # ä½¿ç”¨çœŸå®è¾“å‡ºæ•°æ®
                else:
                    # çœŸå®è¿œç¨‹èŠ‚ç‚¹æ‰§è¡Œ
                    print(f"    å‘é€æ•°æ®åˆ°è¿œç¨‹èŠ‚ç‚¹ {node_id}")

                    # å‘é€æ•°æ®åˆ°è¿œç¨‹èŠ‚ç‚¹
                    transmit_time, output_data = self.send_data_to_node(current_data, node_id, layer_range)
                    total_latency += transmit_time

                    execution_results['node_results'][node_id] = {
                        'execution_time': transmit_time,  # åŒ…å«ä¼ è¾“+æ‰§è¡Œæ—¶é—´
                        'status': 'remote_executed'
                    }
                    current_data = output_data  # ä½¿ç”¨çœŸå®è¾“å‡ºæ•°æ®

                # ä¼ è¾“æ—¶å»¶ï¼ˆä½¿ç”¨çœŸå®æ•°æ®ä¼ è¾“ï¼‰
                if i < len(partition_plan) - 1:
                    next_node = partition_plan[i + 1]['node_id']
                    if next_node != self.node_id:  # åªæœ‰éœ€è¦ä¼ è¾“æ—¶æ‰è®¡ç®—
                        transmit_time = self.calculate_real_transmission_time(current_data, node_id, next_node)
                        total_latency += transmit_time

            # ğŸ¯ å…³é”®ä¿®å¤ï¼šç¡®ä¿ä¿å­˜æœ€ç»ˆè¾“å‡º
            execution_results['final_output'] = current_data

            # ä¼ è¾“æœ€ç»ˆç»“æœåˆ°åœ°é¢ç«™ï¼ˆå¦‚æœæœ‰åœ°é¢ç«™ï¼‰
            if current_data is not None and self.ground_stations:
                ground_station_id = list(self.ground_stations.keys())[0]
                print(f"  ä¼ è¾“æœ€ç»ˆç»“æœåˆ°åœ°é¢ç«™: {ground_station_id}")

                ground_transmit_time = self.calculate_real_transmission_time(current_data, self.node_id,
                                                                             ground_station_id)
                total_latency += ground_transmit_time

                execution_results['ground_station'] = ground_station_id
                execution_results['ground_transmit_time'] = ground_transmit_time
                # æœ€ç»ˆè¾“å‡ºå·²ç»ä¿å­˜åœ¨ä¸Šé¢

            execution_results['total_latency'] = total_latency
            execution_results['success'] = total_latency <= self.max_latency

            print(f"ååŒæ¨ç†å®Œæˆï¼Œæ€»æ—¶å»¶: {total_latency:.2f}ms")

            # ğŸ¯ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æœ€ç»ˆè¾“å‡º
            if execution_results['final_output'] is not None:
                print(f"âœ… æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {execution_results['final_output'].shape}")
            else:
                print("âŒ æœ€ç»ˆè¾“å‡ºä¸ºNone")

        except Exception as e:
            print(f"ååŒæ¨ç†å¤±è´¥: {e}")
            execution_results['success'] = False
            execution_results['error'] = str(e)

        return execution_results

    def send_data_to_node(self, data, remote_node_id, layer_range):
        """å‘é€æ•°æ®åˆ°è¿œç¨‹èŠ‚ç‚¹å¹¶è·å–æ‰§è¡Œç»“æœ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # è·å–è¿œç¨‹èŠ‚ç‚¹ä¿¡æ¯
            node_info = self.neighbor_nodes.get(remote_node_id)
            if not node_info:
                raise ValueError(f"æœªçŸ¥èŠ‚ç‚¹: {remote_node_id}")

            # å»ºç«‹è¿æ¥
            conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            conn.settimeout(60.0)
            conn.connect((node_info['ip'], node_info['port'] + 1000))

            # å‡†å¤‡ä»»åŠ¡æ•°æ® - æ˜ç¡®è¦æ±‚è¿”å›è¾“å‡º
            task_data = {
                'type': 'execute_layers',
                'data': data,
                'layer_range': layer_range,
                'model_type': self.model_type,
                'return_output': True  # ğŸ¯ æ˜ç¡®è¦æ±‚è¿”å›è¾“å‡º
            }

            # å‘é€æ•°æ®å¹¶è®°å½•ä¼ è¾“å¼€å§‹æ—¶é—´
            start_time = time.perf_counter()
            conn.sendall(pickle.dumps(task_data))

            # æ¥æ”¶å“åº”
            response_data = b""
            while True:
                chunk = conn.recv(65536)
                if not chunk:
                    break
                response_data += chunk
                try:
                    response = pickle.loads(response_data)
                    break
                except:
                    continue

            end_time = time.perf_counter()

            response = pickle.loads(response_data)

            if response['status'] == 'success':
                transmit_time = (end_time - start_time) * 1000
                # ğŸ¯ ç¡®ä¿æœ‰è¾“å‡ºæ•°æ®
                if 'output_data' in response and response['output_data'] is not None:
                    return transmit_time, response['output_data']
                else:
                    raise Exception("è¿œç¨‹èŠ‚ç‚¹æ²¡æœ‰è¿”å›è¾“å‡ºæ•°æ®")
            else:
                raise Exception(f"è¿œç¨‹æ‰§è¡Œå¤±è´¥: {response.get('message', 'æœªçŸ¥é”™è¯¯')}")

        except Exception as e:
            print(f"å‘é€æ•°æ®åˆ°èŠ‚ç‚¹ {remote_node_id} å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ä¼°ç®—å€¼
            estimated_time = self.estimate_remote_execution(data, layer_range, remote_node_id)
            return estimated_time, self.estimate_output_size(data, layer_range)
        finally:
            try:
                conn.close()
            except:
                pass

    def calculate_real_transmission_time(self, data, from_node, to_node):
        """è®¡ç®—çœŸå®ä¼ è¾“æ—¶å»¶ï¼ˆåŸºäºæ•°æ®å¤§å°å’Œå¸¦å®½ï¼‰"""
        # è®¡ç®—æ•°æ®å¤§å°
        if torch.is_tensor(data):
            # ğŸ¯ è€ƒè™‘ä¸­é—´ç‰¹å¾å›¾å¯èƒ½æ¯”è¾“å…¥æ›´å¤§
            data_size_bytes = data.nelement() * data.element_size() * 4  # ğŸ¯ å‡è®¾4å€è†¨èƒ€
        else:
            data_size_bytes = len(pickle.dumps(data))

        data_size_mb = data_size_bytes / (1024 * 1024)  # MB

        # è·å–å¸¦å®½ï¼ˆMbpsï¼‰
        bandwidth = self.get_link_bandwidth(from_node, to_node)

        # è®¡ç®—ä¼ è¾“æ—¶é—´ï¼šæ•°æ®å¤§å°(MB) * 8 / å¸¦å®½(Mbps) = æ—¶é—´(ç§’)
        transmission_time_sec = (data_size_mb * 8) / bandwidth

        # è·å–ä¼ æ’­æ—¶å»¶
        propagation_delay = self.get_propagation_delay(from_node, to_node)

        total_time_ms = (transmission_time_sec + propagation_delay) * 1000

        print(f"    ä¼ è¾“æ•°æ®: {data_size_mb:.2f}MB, å¸¦å®½: {bandwidth}Mbps, æ—¶å»¶: {total_time_ms:.2f}ms")
        return total_time_ms

    def get_link_bandwidth(self, from_node, to_node):
        """è·å–é“¾è·¯å¸¦å®½"""
        # æ˜Ÿé—´é“¾è·¯ vs æ˜Ÿåœ°é“¾è·¯
        from_info = self.neighbor_nodes.get(from_node, {})
        to_info = self.neighbor_nodes.get(to_node, {})

        # åˆ¤æ–­æ˜¯å¦ä¸ºæ˜Ÿåœ°é“¾è·¯
        is_ground_link = (from_info.get('type') == 'ground_station' or
                          to_info.get('type') == 'ground_station' or
                          'ground' in from_node.lower() or 'ground' in to_node.lower())

        if is_ground_link:
            return 50.0  # æ˜Ÿåœ°å¸¦å®½è¾ƒä½ 50 Mbps
        else:
            return 200.0  # æ˜Ÿé—´å¸¦å®½è¾ƒé«˜ 200 Mbps

    def get_propagation_delay(self, from_node, to_node):
        """è·å–ä¼ æ’­æ—¶å»¶ï¼ˆç§’ï¼‰"""
        # æ˜Ÿé—´é“¾è·¯ vs æ˜Ÿåœ°é“¾è·¯
        from_info = self.neighbor_nodes.get(from_node, {})
        to_info = self.neighbor_nodes.get(to_node, {})

        is_ground_link = (from_info.get('type') == 'ground_station' or
                          to_info.get('type') == 'ground_station')

        if is_ground_link:
            return 0.27  # ä½è½¨å«æ˜Ÿåˆ°åœ°é¢ç«™çº¦270ms
        else:
            return 0.01  # æ˜Ÿé—´é“¾è·¯çº¦10ms

    def extract_layers_range(self, start_layer, end_layer):
        """æå–æŒ‡å®šå±‚èŒƒå›´çš„æ¨¡å‹ï¼ˆé€‚é…è‡ªå®šä¹‰æ¨¡å‹ç»“æ„ï¼‰"""
        layers = []
        for i in range(start_layer, end_layer):
            layer = self.current_model[i]  # ä½¿ç”¨æ¨¡å‹è‡ªå®šä¹‰çš„ __getitem__ æ–¹æ³•
            layers.append(layer)

        # å°†å±‚åºåˆ—åŒ…è£…æˆ Sequential
        return torch.nn.Sequential(*layers)

    def estimate_remote_execution(self, data, layer_range, remote_node_id):
        """ä¼°è®¡è¿œç¨‹èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´"""
        # ç®€åŒ–ä¼°è®¡ï¼ŒåŸºäºèŠ‚ç‚¹ç®—åŠ›å’Œå±‚æ•°
        node_info = self.neighbor_nodes.get(remote_node_id, {})
        compute_capacity = node_info.get('compute_capacity', 4.0)
        num_layers = layer_range[1] - layer_range[0]

        base_time = num_layers * 15.0  # æ¯å±‚åŸºç¡€æ—¶é—´
        adjusted_time = base_time * (8.0 / compute_capacity)  # åŸºäºç®—åŠ›è°ƒæ•´

        return adjusted_time

    def estimate_transmission_time(self, data, from_node, to_node):
        """ä¼°è®¡æ•°æ®ä¼ è¾“æ—¶é—´"""
        # ç®€åŒ–ä¼°è®¡
        if from_node == self.node_id or to_node == self.node_id:
            return 5.0  # ms
        else:
            return 10.0  # ms

    def estimate_ground_transmission_time(self, data):
        """ä¼°ç®—åˆ°åœ°é¢ç«™çš„ä¼ è¾“æ—¶å»¶"""
        # å«æ˜Ÿåˆ°åœ°é¢ç«™çš„ä¼ è¾“é€šå¸¸æœ‰æ›´é«˜çš„å»¶è¿Ÿ
        if torch.is_tensor(data):
            data_size = data.nelement() * 4 / (1024 * 1024)  # MB
        else:
            data_size = 1.0

        # åœ°é¢ç«™å¸¦å®½é€šå¸¸è¾ƒé«˜ï¼Œä½†å»¶è¿Ÿä¹Ÿè¾ƒé«˜
        bandwidth = 200.0  # Mbps
        transmission_time = (data_size * 8) / bandwidth  # ç§’
        base_latency = 50.0  # åŸºç¡€å«æ˜Ÿåˆ°åœ°é¢å»¶è¿Ÿ

        return transmission_time * 1000 + base_latency  # è½¬æ¢ä¸ºæ¯«ç§’

    def generate_partition_strategies(self, total_layers, available_nodes):
        """ç”Ÿæˆå¯èƒ½çš„åˆ†å‰²ç­–ç•¥"""
        strategies = []
        num_nodes = len(available_nodes)

        # ç­–ç•¥1ï¼šå‡åŒ€åˆ†é…
        layers_per_node = max(1, total_layers // num_nodes)
        uniform_plan = []
        current_layer = 0

        for i, node_id in enumerate(available_nodes):
            if current_layer >= total_layers:
                break
            end_layer = min(current_layer + layers_per_node, total_layers)
            uniform_plan.append({
                'node_id': node_id,
                'layer_range': (current_layer, end_layer),
                'compute_load': end_layer - current_layer
            })
            current_layer = end_layer
        strategies.append(uniform_plan)

        # ç­–ç•¥2ï¼šåŸºäºç®—åŠ›åŠ æƒåˆ†é…
        weighted_plan = []
        current_layer = 0

        # è®¡ç®—æ€»ç®—åŠ›
        total_compute = sum(1.0 for _ in available_nodes)  # ç®€åŒ–

        for i, node_id in enumerate(available_nodes):
            if current_layer >= total_layers:
                break
            # åŸºäºç®—åŠ›åˆ†é…å±‚æ•°
            compute_ratio = 1.0 / num_nodes  # ç®€åŒ–
            layers_to_assign = max(1, int(total_layers * compute_ratio))
            end_layer = min(current_layer + layers_to_assign, total_layers)
            weighted_plan.append({
                'node_id': node_id,
                'layer_range': (current_layer, end_layer),
                'compute_load': end_layer - current_layer
            })
            current_layer = end_layer
        strategies.append(weighted_plan)

        return strategies

    def predict_strategy_latency(self, partition_plan, network_info):
        """é¢„æµ‹åˆ†å‰²ç­–ç•¥çš„æ€»æ—¶å»¶"""
        total_latency = 0.0
        current_data = self.input_data

        for i, partition in enumerate(partition_plan):
            node_id = partition['node_id']
            layer_range = partition['layer_range']

            # ä½¿ç”¨ä½ ç°æœ‰çš„æ—¶å»¶é¢„æµ‹æ¨¡å‹
            if node_id == self.node_id:
                # æœ¬èŠ‚ç‚¹æ‰§è¡Œ
                comp_latency = self.predict_computation_latency(current_data, layer_range)
            else:
                # å…¶ä»–èŠ‚ç‚¹æ‰§è¡Œï¼ˆç®€åŒ–é¢„æµ‹ï¼‰
                node_info = network_info['nodes'][node_id]
                comp_latency = self.estimate_remote_computation(layer_range, node_info)

            total_latency += comp_latency

            # é¢„æµ‹ä¼ è¾“æ—¶å»¶
            if i < len(partition_plan) - 1:
                next_node = partition_plan[i + 1]['node_id']
                transmit_latency = self.predict_transmission_latency(current_data, node_id, next_node, network_info)
                total_latency += transmit_latency

            # æ›´æ–°æ•°æ®å¤§å°ä¼°è®¡ï¼ˆç®€åŒ–ï¼‰
            current_data = self.estimate_output_size(current_data, layer_range)

        return total_latency

    def predict_computation_latency(self, input_data, layer_range):
        """ä½¿ç”¨é¢„è®­ç»ƒçš„æ—¶å»¶é¢„æµ‹æ¨¡å‹é¢„æµ‹è®¡ç®—æ—¶å»¶"""
        start_layer, end_layer = layer_range
        target_model = self.extract_layers_range(start_layer, end_layer)

        # ğŸ†• ç¡®ä¿ä½¿ç”¨æ‚¨çš„é¢„è®­ç»ƒé¢„æµ‹å™¨
        predicted_latency = predictor_utils.predict_model_latency(
            input_data, target_model, self.device, self.predictor_dict
        )

        print(f"    æ—¶å»¶é¢„æµ‹: {predicted_latency:.2f}ms (å±‚{start_layer}-{end_layer})")
        return predicted_latency

    def estimate_remote_computation(self, layer_range, node_info):
        """ä¼°ç®—è¿œç¨‹èŠ‚ç‚¹çš„è®¡ç®—æ—¶å»¶"""
        start_layer, end_layer = layer_range
        num_layers = end_layer - start_layer

        # åŸºäºèŠ‚ç‚¹ç®—åŠ›å’Œè®¾å¤‡ç±»å‹ä¼°ç®—
        base_time_per_layer = 10.0  # ms
        device_factor = 1.0 if node_info['device'] == "cpu" else 0.3
        compute_capacity_factor = 8.0 / node_info['compute_capacity']  # ç›¸å¯¹äºåŸºå‡†ç®—åŠ›8.0

        estimated_time = num_layers * base_time_per_layer * device_factor * compute_capacity_factor
        return estimated_time

    def predict_transmission_latency(self, data, from_node, to_node, network_info):
        """é¢„æµ‹ä¼ è¾“æ—¶å»¶"""
        # ä¼°ç®—æ•°æ®å¤§å°
        if torch.is_tensor(data):
            data_size = data.nelement() * 4 / (1024 * 1024)  # MB
        else:
            data_size = 1.0  # é»˜è®¤1MB

        # è·å–é“¾è·¯å¸¦å®½
        link_key = f"{from_node}->{to_node}"
        bandwidth = network_info['links'].get(link_key, {}).get('bandwidth', 50.0)  # Mbps

        # è®¡ç®—ä¼ è¾“æ—¶é—´
        transmission_time = (data_size * 8) / bandwidth  # ç§’
        return transmission_time * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

    def estimate_output_size(self, input_data, layer_range):
        """ä¼°ç®—å±‚è¾“å‡ºæ•°æ®å¤§å°ï¼ˆç®€åŒ–ï¼‰"""
        # è¿”å›æ¨¡æ‹Ÿçš„è¾“å‡ºæ•°æ®ï¼ˆä¿æŒå½¢çŠ¶ï¼‰
        return torch.rand_like(input_data)

    def execute_assigned_layers(self, input_data, layer_range):
        """æ‰§è¡Œåˆ†é…åˆ°çš„æ¨¡å‹å±‚ - ä¿®å¤è®¾å¤‡ä¸åŒ¹é…"""
        print(f"    {self.node_id} æ‰§è¡Œå±‚ {layer_range}")

        # ç¡®å®šç›®æ ‡è®¾å¤‡
        target_device = self.device
        if target_device == "cuda" and not torch.cuda.is_available():
            target_device = "cpu"
            print(f"âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°{target_device}")

        # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
        if str(input_data.device) != target_device:
            input_data = input_data.to(target_device)
            print(f"ğŸ”„ ç§»åŠ¨è¾“å…¥æ•°æ®åˆ°: {target_device}")

        # æå–æŒ‡å®šå±‚èŒƒå›´çš„æ¨¡å‹
        start_layer, end_layer = layer_range
        target_model = self.extract_layers_range(start_layer, end_layer)

        # ç¡®ä¿æ¨¡å‹åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
        target_model = target_model.to(target_device)
        print(f"ğŸ”„ æ¨¡å‹å·²ç§»åŠ¨åˆ°: {target_device}")

        # é¢„çƒ­
        self.warm_up_model(target_model, input_data)

        # é«˜ç²¾åº¦è®¡æ—¶
        num_runs = 10
        execution_times = []

        # é¢„çƒ­ï¼ˆä¸è®¡æ—¶ï¼‰
        for _ in range(3):
            with torch.no_grad():
                _ = target_model(input_data)

        # æ­£å¼è®¡æ—¶è¿è¡Œ
        for run in range(num_runs):
            start_time = time.perf_counter()
            with torch.no_grad():
                output_data = target_model(input_data)
            end_time = time.perf_counter()
            execution_times.append((end_time - start_time) * 1000)

        # å–ä¸­ä½æ•°é¿å…å¼‚å¸¸å€¼
        execution_times.sort()
        median_time = execution_times[len(execution_times) // 2]

        print(f"       æ‰§è¡Œå®Œæˆ, ä¸­ä½æ—¶å»¶: {median_time:.3f}ms")
        return output_data, median_time

    def warm_up_model(self, model, input_data):
        """æ¨¡å‹é¢„çƒ­"""
        model.eval()
        if self.device == "cuda" and torch.cuda.is_available():
            model = model.cuda()
            input_data = input_data.cuda()

        # é¢„çƒ­è¿è¡Œ
        with torch.no_grad():
            for _ in range(3):
                _ = model(input_data)

    def start_task_service(self):
        """å¯åŠ¨ä»»åŠ¡æ¥æ”¶æœåŠ¡"""

        def task_handler():
            try:
                server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                server_socket.bind((self.ip, self.port + 1000))  # ä»»åŠ¡æœåŠ¡ä½¿ç”¨ä¸åŒç«¯å£
                server_socket.listen(5)

                print(f"{self.node_id} ä»»åŠ¡æœåŠ¡å¯åŠ¨åœ¨ {self.ip}:{self.port + 1000}")

                while True:
                    conn, addr = server_socket.accept()
                    threading.Thread(
                        target=self._handle_task_connection,
                        args=(conn, addr)
                    ).start()

            except Exception as e:
                print(f"ä»»åŠ¡æœåŠ¡é”™è¯¯: {e}")

        thread = threading.Thread(target=task_handler, daemon=True)
        thread.start()
        return thread

    def _handle_task_connection(self, conn, addr):
        """å¤„ç†ä»»åŠ¡è¿æ¥è¯·æ±‚"""
        try:
            conn.settimeout(120.0)  # ğŸ¯ å¢å¤§è¶…æ—¶åˆ°120ç§’
            data = b""

            # ğŸ¯ å¢å¤§ç¼“å†²åŒºå¹¶ç¡®ä¿å®Œæ•´æ¥æ”¶
            while True:
                chunk = conn.recv(65536)  # ğŸ¯ å¢å¤§åˆ°64KBæ¯æ¬¡
                if not chunk:
                    break
                data += chunk
                # å°è¯•åˆ¤æ–­æ•°æ®æ˜¯å¦å®Œæ•´
                try:
                    message = pickle.loads(data)
                    break  # æˆåŠŸè§£æï¼Œæ•°æ®å®Œæ•´
                except:
                    continue  # æ•°æ®ä¸å®Œæ•´ï¼Œç»§ç»­æ¥æ”¶

            if not data:
                return

            message = pickle.loads(data)

            if message['type'] == 'task':
                print(f"{self.node_id} æ¥æ”¶åˆ°ä»»åŠ¡: {message['data']['task_id']}")

                if self.satellite_type == SatelliteType.REMOTE_SENSING and not self.is_busy:
                    result = self.assign_task(message['data'])
                else:
                    result = {"status": "not_coordinator", "message": "éåè°ƒèŠ‚ç‚¹"}

                # è¿”å›ç»“æœ
                conn.send(pickle.dumps(result))


            elif message['type'] == 'execute_layers':

                print(f"{self.node_id} æ¥æ”¶åˆ°å±‚æ‰§è¡Œè¯·æ±‚: {message['layer_range']}")

                # åŠ è½½æ¨¡å‹

                if not self.current_model:
                    self.current_model = get_dnn_model(message['model_type'])

                # æ‰§è¡ŒæŒ‡å®šå±‚

                output_data, exec_time = self.execute_assigned_layers(

                    message['data'],

                    message['layer_range']

                )

                # ğŸ¯ ç¡®ä¿è¿”å›è¾“å‡ºæ•°æ®

                response = {

                    'status': 'success',

                    'output_data': output_data,  # ç¡®ä¿æœ‰è¿™ä¸ªå­—æ®µ

                    'execution_time': exec_time

                }

                conn.send(pickle.dumps(response))

            else:
                conn.send(pickle.dumps({"status": "error", "message": "æœªçŸ¥æ¶ˆæ¯ç±»å‹"}))

        except Exception as e:
            print(f"å¤„ç†ä»»åŠ¡æ¶ˆæ¯é”™è¯¯: {e}")
            try:
                conn.send(pickle.dumps({"status": "error", "message": str(e)}))
            except:
                pass
        finally:
            conn.close()

    def evaluate_local_model(self, testloader):
        """è¯„ä¼°æœ¬åœ°æ¨¡å‹ç²¾åº¦"""
        if not self.current_model:
            print(f"{self.node_id} æ²¡æœ‰åŠ è½½æ¨¡å‹")
            return 0.0

        accuracy = evaluate_model_accuracy(self.current_model, testloader, self.device)
        print(f"{self.node_id} æ¨¡å‹ç²¾åº¦: {accuracy:.2f}%")
        return accuracy

    def execute_complete_model(self, input_data):
        """æ‰§è¡Œå®Œæ•´æ¨¡å‹ï¼ˆç”¨äºå•æ˜Ÿæ¨ç†å¯¹æ¯”ï¼‰"""
        if not self.current_model:
            print(f"{self.node_id} æ²¡æœ‰åŠ è½½æ¨¡å‹")
            return None, 0.0

        print(f"{self.node_id} æ‰§è¡Œå®Œæ•´æ¨¡å‹æ¨ç†...")

        # é«˜ç²¾åº¦è®¡æ—¶
        num_runs = 10
        execution_times = []

        # é¢„çƒ­
        self.warm_up_model(self.current_model, input_data)

        # æ­£å¼è®¡æ—¶
        for run in range(num_runs):
            start_time = time.perf_counter()
            with torch.no_grad():
                output_data = self.current_model(input_data)
            end_time = time.perf_counter()
            execution_times.append((end_time - start_time) * 1000)

        execution_times.sort()
        median_time = execution_times[len(execution_times) // 2]

        print(f"    å®Œæ•´æ¨¡å‹æ‰§è¡Œå®Œæˆ, æ—¶å»¶: {median_time:.2f}ms")
        return output_data, median_time