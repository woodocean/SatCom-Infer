import torch
import torch.nn as nn
import time
import sys
import os

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.LeNet import LeNet
from models.AlexNet import AlexNet
from models.VggNet import vgg16_bn
from models.MobileNet import MobileNet
from .excel_utils import *


# ä¿®æ”¹ inference_utils.py ä¸­çš„ get_dnn_model å‡½æ•°
def get_dnn_model(arg: str):
    """
    è·å–DNNæ¨¡å‹
    :param arg: æ¨¡å‹åå­—
    :return: å¯¹åº”çš„æ¨¡å‹
    """
    input_channels = 3
    if arg == "alex_net":
        # åˆ›å»ºæ¨¡å‹
        model = AlexNet(input_channels=input_channels, num_classes=10)

        # åŠ è½½ç¬¬115ä¸ªepochçš„æ£€æŸ¥ç‚¹
        checkpoint_path = 'checkpoints/alexnet_cifar10_epoch_115.pth'
        if os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

                # åŠ è½½æ¨¡å‹æƒé‡
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()

                print(f"âœ… æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹æ¨¡å‹: {checkpoint_path}")
                print(f"ğŸ“Š è®­ç»ƒç²¾åº¦: {checkpoint.get('train_accuracy', 'N/A')}%")
                print(f"ğŸ“Š æµ‹è¯•ç²¾åº¦: {checkpoint.get('test_accuracy', 'N/A')}%")

            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

        return model

    elif arg == "vgg_net":
        return vgg16_bn(input_channels=input_channels)
    elif arg == "le_net":
        return LeNet(input_channels=input_channels)
    elif arg == "mobile_net":
        return MobileNet(input_channels=input_channels)
    else:
        raise RuntimeError("æ²¡æœ‰å¯¹åº”çš„DNNæ¨¡å‹")


def model_partition(model, index):
    """
    model_partitionå‡½æ•°å¯ä»¥å°†ä¸€ä¸ªæ•´ä½“çš„model,åˆ’åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†
    åˆ’åˆ†çš„å¤§è‡´æ€è·¯ï¼š
        å¦‚é€‰å®šç¬¬indexå±‚å¯¹æ¨¡å‹è¿›è¡Œåˆ’åˆ† ï¼Œåˆ™ä»£è¡¨åœ¨ç¬¬indexåå¯¹æ¨¡å‹è¿›è¡Œåˆ’åˆ†
        å°†indexå±‚ä¹‹å‰çš„å±‚åŒ…æ‹¬ç¬¬indexå±‚ - å°è£…è¿›edge_modelä¸­äº¤ç»™è¾¹ç¼˜ç«¯è®¾å¤‡æ¨ç†
        å°†indexä¹‹åçš„å±‚ - å°è£…è¿›cloud_modeläº¤ç»™äº‘ç«¯è®¾å¤‡æ¨ç†
    ä¸¾ä¾‹ï¼šåœ¨ç¬¬indexå±‚ä¹‹åå¯¹æ¨¡å‹è¿›è¡Œåˆ’åˆ†
    index = 0 - ä»£è¡¨åœ¨åˆå§‹è¾“å…¥è¿›è¡Œåˆ’åˆ†
    index = 1 - ä»£è¡¨åœ¨ç¬¬1å±‚åå¯¹æ¨¡å‹è¿›è¡Œåˆ’åˆ†,edge_cloudåŒ…æ‹¬ç¬¬1å±‚

    :param model: ä¼ å…¥æ¨¡å‹
    :param index: æ¨¡å‹åˆ’åˆ†ç‚¹
    :return: åˆ’åˆ†ä¹‹åçš„è¾¹ç«¯æ¨¡å‹å’Œäº‘ç«¯æ¨¡å‹
    """
    edge_model = nn.Sequential()
    cloud_model = nn.Sequential()
    idx = 1
    for layer in model:
        if idx <= index:
            edge_model.add_module(f"{idx}-{layer.__class__.__name__}", layer)
        else:
            cloud_model.add_module(f"{idx}-{layer.__class__.__name__}", layer)
        idx += 1
    return edge_model, cloud_model



def show_model_constructor(model,skip=True):
    """
    å±•ç¤ºDNNå„å±‚ç»“æ„
    :param model: DNNæ¨¡å‹
    :param skip: æ˜¯å¦éœ€è¦è·³è¿‡ ReLU BatchNorm Dropoutç­‰å±‚
    :return: å±•ç¤ºDNNå„å±‚ç»“æ„
    """
    print("show model constructor as follows: ")
    if len(model) > 0:
        idx = 1
        for layer in model:
            if skip is True:
                if isinstance(layer, nn.ReLU) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.Dropout):
                    continue
            print(f'{idx}-{layer}')
            idx += 1
    else:
        print("this model is a empty model")



def show_features(model, input_data, device, epoch_cpu=50, epoch_gpu=100, skip=True, save=False, sheet_name="model", path=None):
    """
    å¯ä»¥è¾“å‡ºDNNå„å±‚çš„æ€§è´¨,å¹¶å°†å…¶ä¿å­˜åœ¨excelè¡¨æ ¼ä¸­,è¾“å‡ºçš„ä¸»è¦æ€§è´¨å¦‚ä¸‹ï¼š
    ["index", "layerName", "computation_time(ms)", "output_shape", "transport_num", "transport_size(MB)","accumulate_time(ms)"]
    [DNNå±‚ä¸‹æ ‡ï¼Œå±‚åå­—ï¼Œå±‚è®¡ç®—æ—¶å»¶ï¼Œå±‚è¾“å‡ºå½¢çŠ¶ï¼Œéœ€è¦ä¼ è¾“çš„æµ®ç‚¹æ•°æ•°é‡ï¼Œä¼ è¾“å¤§å°ï¼Œä»ç¬¬1å±‚å¼€å§‹çš„ç´¯è®¡æ¨ç†æ—¶å»¶]
    :param model: DNNæ¨¡å‹
    :param input_data: è¾“å…¥æ•°æ®
    :param device: æŒ‡å®šè¿è¡Œè®¾å¤‡
    :param epoch_cpu: cpuå¾ªç¯æ¨ç†æ¬¡æ•°
    :param epoch_gpu: gpuå¾ªç¯æ¨ç†æ¬¡æ•°
    :param skip: æ˜¯å¦è·³è¿‡ä¸é‡è¦çš„DNNå±‚
    :param save: æ˜¯å¦å°†å†…å®¹ä¿å­˜åœ¨excelè¡¨æ ¼ä¸­
    :param sheet_name: excelä¸­çš„è¡¨æ ¼åå­—
    :param path: excelè·¯å¾„
    :return: None
    """
    if device == "cuda":
        if not torch.torch.cuda.is_available():
            raise RuntimeError("è¿è¡Œè®¾å¤‡ä¸Šæ²¡æœ‰cuda è¯·è°ƒæ•´deviceå‚æ•°ä¸ºcpu")

    # æ¨ç†ä¹‹å‰å¯¹è®¾å¤‡è¿›è¡Œé¢„çƒ­
    warmUp(model, input_data, device)

    if save:
        sheet_name = sheet_name
        value = [["index", "layerName", "computation_time(ms)", "output_shape", "transport_num",
                  "transport_size(MB)", "accumulate_time(ms)"]]
        create_excel_xsl(path, sheet_name, value)


    if len(model) > 0:
        idx = 1
        accumulate_time = 0.0
        for layer in model:
            if skip is True:
                if isinstance(layer, nn.ReLU) or isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.Dropout):
                    continue

            temp_x = input_data
            # è®°å½•DNNå•å±‚çš„æ¨ç†æ—¶é—´
            input_data, layer_time = recordTime(layer, temp_x, device, epoch_cpu, epoch_gpu)
            accumulate_time += layer_time

            # è®¡ç®—ä¸­é—´ä¼ è¾“å ç”¨å¤§å°ä¸ºå¤šå°‘MB
            total_num = 1
            for num in input_data.shape:
                total_num *= num
            size = total_num * 4 / 1000 / 1000

            print("------------------------------------------------------------------")
            print(f'{idx}-{layer} \n'
                  f'computation time: {layer_time :.3f} ms\n'
                  f'output shape: {input_data.shape}\t transport_num:{total_num}\t transport_size:{size:.3f}MB\t accumulate time:{accumulate_time:.3f}ms\n')

            # ä¿å­˜åˆ°excelè¡¨æ ¼ä¸­
            if save:
                sheet_name = input_data
                value = [[idx, f"{layer}", round(layer_time, 3), f"{input_data.shape}", total_num, round(size, 3),
                          round(accumulate_time, 3)]]
                write_excel_xls_append(path, sheet_name, value)
            idx += 1
        return input_data
    else:
        print("this model is a empty model")
        return input_data



def warmUp(model,input_data,device):
    """
    é¢„çƒ­æ“ä½œï¼šä¸å¯¹è®¾å¤‡è¿›è¡Œé¢„çƒ­çš„è¯ï¼Œæ”¶é›†çš„æ•°æ®ä¼šæœ‰æ—¶å»¶åå·®
    :param model: DNNæ¨¡å‹
    :param input_data: è¾“å…¥æ•°æ®
    :param device: è¿è¡Œè®¾å¤‡ç±»å‹
    :return: None
    """
    epoch = 10
    model = model.to(device)
    for i in range(1):
        if device == "cuda":
            warmUpGpu(model, input_data, device, epoch)
        elif device == "cpu":
            warmUpCpu(model, input_data, device, epoch)


def warmUpGpu(model, input_data, device, epoch):
    """ GPU è®¾å¤‡é¢„çƒ­"""
    dummy_input = torch.rand(input_data.shape).to(device)
    with torch.no_grad():
        for i in range(10):
            _ = model(dummy_input)

        avg_time = 0.0
        for i in range(epoch):
            starter = torch.cuda.Event(enable_timing=True)
            ender = torch.cuda.Event(enable_timing=True)
            starter.record()

            _ = model(dummy_input)

            ender.record()
            torch.cuda.synchronize()
            curr_time = starter.elapsed_time(ender)
            avg_time += curr_time
        avg_time /= epoch
        # print(f"GPU Warm Up : {curr_time:.3f}ms")
        # print("==============================================")


def warmUpCpu(model, input_data, device, epoch):
    """ CPU è®¾å¤‡é¢„çƒ­"""
    dummy_input = torch.rand(input_data.shape).to(device)
    with torch.no_grad():
        for i in range(10):
            _ = model(dummy_input)

        avg_time = 0.0
        for i in range(epoch):
            start = time.perf_counter()
            _ = model(dummy_input)
            end = time.perf_counter()
            curr_time = end - start
            avg_time += curr_time
        avg_time /= epoch
        # print(f"CPU Warm Up : {curr_time * 1000:.3f}ms")
        # print("==============================================")



def recordTime(model,input_data,device,epoch_cpu,epoch_gpu):
    """
    è®°å½•DNNæ¨¡å‹æˆ–è€…DNNå±‚çš„æ¨ç†æ—¶é—´ æ ¹æ®è®¾å¤‡åˆ†å‘åˆ°ä¸åŒå‡½æ•°ä¸Šè¿›è¡Œè®¡ç®—
    :param model: DNNæ¨¡å‹
    :param input_data: è¾“å…¥æ•°æ®
    :param device: è¿è¡Œè®¾å¤‡
    :param epoch_cpu: cpuå¾ªç¯æ¨ç†æ¬¡æ•°
    :param epoch_gpu: gpuå¾ªç¯æ¨ç†æ¬¡æ•°
    :return: è¾“å‡ºç»“æœä»¥åŠæ¨ç†æ—¶å»¶
    """
    model = model.to(device)
    res_x, computation_time = None, None
    if device == "cuda":
        res_x, computation_time = recordTimeGpu(model, input_data, device, epoch_gpu)
    elif device == "cpu":
        res_x, computation_time = recordTimeCpu(model, input_data, device, epoch_cpu)
    return res_x, computation_time



def recordTimeGpu(model, input_data, device, epoch):
    all_time = 0.0
    with torch.no_grad():
        for i in range(epoch):
            if torch.is_tensor(input_data):
                input_data = torch.rand(input_data.shape).to(device)
            # init loggers
            starter = torch.cuda.Event(enable_timing=True)
            ender = torch.cuda.Event(enable_timing=True)

            with torch.no_grad():
                starter.record()
                res_x = model(input_data)
                ender.record()

            # wait for GPU SYNC
            # å…³äºGPUçš„è®¡ç®—æœºåˆ¶ ä¸€å®šè¦æœ‰ä¸‹é¢è¿™ä¸€è¡Œæ‰èƒ½å‡†ç¡®æµ‹é‡åœ¨GPUä¸Šçš„æ¨ç†æ—¶å»¶
            torch.cuda.synchronize()
            curr_time = starter.elapsed_time(ender)
            all_time += curr_time
        all_time /= epoch
    return res_x, all_time


def recordTimeCpu(model, input_data, device, epoch):
    all_time = 0.0
    for i in range(epoch):
        if torch.is_tensor(input_data):
            input_data = torch.rand(input_data.shape).to(device)

        with torch.no_grad():
            start_time = time.perf_counter()
            res_x = model(input_data)
            end_time = time.perf_counter()

        curr_time = end_time - start_time
        all_time += curr_time
    all_time /= epoch
    return res_x, all_time * 1000
