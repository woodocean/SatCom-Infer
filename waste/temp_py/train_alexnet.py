import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import time
import os
from models.AlexNet import AlexNet  # å¯¼å…¥ä½ çš„AlexNetæ¨¡å‹


def train_alexnet_cifar10(resume_epoch=None, total_epochs=80):
    """è®­ç»ƒAlexNetåœ¨CIFAR-10æ•°æ®é›†ä¸Š - æ”¯æŒç»§ç»­è®­ç»ƒåˆ°æŒ‡å®šæ€»epochæ•°"""

    # è®­ç»ƒå‚æ•°
    batch_size = 256
    num_classes = 10

    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ•°æ®é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½CIFAR-10æ•°æ®é›†...")
    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )

    if len(trainset) > 25000:
        from torch.utils.data import Subset
        trainset = Subset(trainset, range(25000))

    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    testset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

    # åˆ›å»ºæ¨¡å‹
    model = AlexNet(input_channels=3, num_classes=num_classes)
    model = model.to(device)

    # ä½¿ç”¨DataParallelå¦‚æœæœ‰å¤šGPU
    if torch.cuda.device_count() > 1:
        print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        model = nn.DataParallel(model)

    # åˆå§‹åŒ–è®­ç»ƒå˜é‡
    start_epoch = 0
    train_losses = []
    train_accuracies = []
    test_accuracies = []

    # å­¦ä¹ ç‡è®¾ç½®
    if resume_epoch is not None:
        learning_rate = 0.005  # ç»§ç»­è®­ç»ƒæ—¶ç”¨æ›´å°çš„å­¦ä¹ ç‡
    else:
        learning_rate = 0.01

    # å¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤
    if resume_epoch is not None:
        checkpoint_path = f'../../checkpoints/alexnet_cifar10_epoch_{resume_epoch}.pth'
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)

            # åŠ è½½æ¨¡å‹çŠ¶æ€
            if isinstance(model, nn.DataParallel):
                model.module.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint['model_state_dict'])

            # åŠ è½½è®­ç»ƒå†å²
            start_epoch = checkpoint['epoch'] + 1
            train_losses = checkpoint.get('train_losses', [])
            train_accuracies = checkpoint.get('train_accuracies', [])
            test_accuracies = checkpoint.get('test_accuracies', [])

            print(f"âœ… ä»epoch {resume_epoch}æ¢å¤è®­ç»ƒï¼Œç›®æ ‡epoch: {total_epochs}")
            print(f"   ä¹‹å‰è®­ç»ƒç²¾åº¦: {checkpoint['train_accuracy']:.2f}%")
            print(f"   ä¹‹å‰æµ‹è¯•ç²¾åº¦: {checkpoint['test_accuracy']:.2f}%")
            print(f"   ä½¿ç”¨å­¦ä¹ ç‡: {learning_rate}")
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹ {checkpoint_path} ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            resume_epoch = None

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)  # è°ƒæ•´å­¦ä¹ ç‡è¡°å‡æ­¥é•¿

    print("å¼€å§‹è®­ç»ƒ...")
    print(f"{'Epoch':^6} | {'Train Loss':^12} | {'Train Acc':^10} | {'Test Acc':^10} | {'Time':^8}")
    print("-" * 60)

    for epoch in range(start_epoch, total_epochs):
        start_time = time.time()

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (inputs, targets) in enumerate(trainloader):
            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)

            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            if batch_idx % 50 == 0:
                current_acc = 100. * correct / total
                print(
                    f'Epoch: {epoch + 1}/{total_epochs} | Batch: {batch_idx}/{len(trainloader)} | Loss: {loss.item():.4f} | Acc: {current_acc:.2f}%')

        # è®¡ç®—è®­ç»ƒç²¾åº¦
        train_loss = running_loss / len(trainloader)
        train_acc = 100. * correct / total

        # æµ‹è¯•é˜¶æ®µ - æ¯ä¸ªepochéƒ½æµ‹è¯•
        test_acc = evaluate_model(model, testloader, device)

        # è®°å½•å†å²
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        test_accuracies.append(test_acc)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]

        # æ‰“å°è¿›åº¦
        epoch_time = time.time() - start_time
        print(f"{epoch + 1:^6} | {train_loss:^12.4f} | {train_acc:^10.2f}% | {test_acc:^10.2f}% | {epoch_time:^8.2f}s")
        print(f"      å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")

        # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (epoch + 1) % 5 == 0 or epoch == total_epochs - 1:
            save_checkpoint_with_history(model, epoch, train_acc, test_acc, train_losses, train_accuracies,
                                         test_accuracies)

    print("è®­ç»ƒå®Œæˆ!")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_final_model(model, test_acc)

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curve(train_losses, train_accuracies, test_accuracies)

    return model, train_losses, train_accuracies, test_accuracies

def save_checkpoint_with_history(model, epoch, train_acc, test_acc, train_losses, train_accuracies, test_accuracies):
    """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«è®­ç»ƒå†å²ï¼‰"""
    # å¦‚æœæ˜¯DataParallelï¼Œä¿å­˜åŸå§‹æ¨¡å‹
    if isinstance(model, nn.DataParallel):
        model_state_dict = model.module.state_dict()
    else:
        model_state_dict = model.state_dict()

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model_state_dict,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc,
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'test_accuracies': test_accuracies
    }

    os.makedirs('../../checkpoints', exist_ok=True)
    filename = f'../../checkpoints/alexnet_cifar10_epoch_{epoch + 1}.pth'
    torch.save(checkpoint, filename)
    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")


def evaluate_model(model, testloader, device):
    """å¿«é€Ÿè¯„ä¼°æ¨¡å‹ç²¾åº¦"""
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        # åªè¯„ä¼°éƒ¨åˆ†æµ‹è¯•æ•°æ®ä»¥èŠ‚çœæ—¶é—´
        for i, (inputs, targets) in enumerate(testloader):
            if i >= 20:  # åªè¯„ä¼°20ä¸ªæ‰¹æ¬¡
                break
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    accuracy = 100. * correct / total
    return accuracy


def save_checkpoint(model, epoch, train_acc, test_acc):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    # å¦‚æœæ˜¯DataParallelï¼Œä¿å­˜åŸå§‹æ¨¡å‹
    if isinstance(model, nn.DataParallel):
        model_state_dict = model.module.state_dict()
    else:
        model_state_dict = model.state_dict()

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model_state_dict,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc
    }

    os.makedirs('../../checkpoints', exist_ok=True)
    filename = f'../../checkpoints/alexnet_cifar10_epoch_{epoch + 1}.pth'
    torch.save(checkpoint, filename)
    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")


def save_final_model(model, test_acc):
    """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
    os.makedirs('../../trained_models', exist_ok=True)

    # å¦‚æœæ˜¯DataParallelï¼Œä¿å­˜åŸå§‹æ¨¡å‹
    if isinstance(model, nn.DataParallel):
        model_state_dict = model.module.state_dict()
    else:
        model_state_dict = model.state_dict()

    # ä¿å­˜å®Œæ•´æ¨¡å‹
    model_path = '../../trained_models/alexnet_cifar10_final.pth'
    torch.save(model_state_dict, model_path)
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {model_path}, æµ‹è¯•ç²¾åº¦: {test_acc:.2f}%")

    # ä¿å­˜ç”¨äºå«æ˜Ÿç³»ç»Ÿçš„æ¨¡å‹
    satellite_model_path = '../../trained_models/alexnet_cifar10_satellite.pth'
    torch.save({
        'model_state_dict': model_state_dict,
        'num_classes': 10,
        'input_channels': 3,
        'test_accuracy': test_acc
    }, satellite_model_path)
    print(f"å«æ˜Ÿç³»ç»Ÿæ¨¡å‹å·²ä¿å­˜: {satellite_model_path}")


def plot_training_curve(train_losses, train_accuracies, test_accuracies):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        import matplotlib.pyplot as plt

        plt.figure(figsize=(12, 4))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(train_losses)
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)

        # ç»˜åˆ¶ç²¾åº¦æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.plot(train_accuracies, label='Train Accuracy')
        plt.plot(test_accuracies, label='Test Accuracy')
        plt.title('Training and Test Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('training_curve.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_curve.png")

    except ImportError:
        print("Matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜åˆ¶è®­ç»ƒæ›²çº¿")


def test_trained_model():
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("\næµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹...")

    # åŠ è½½æ¨¡å‹
    model = AlexNet(input_channels=3, num_classes=10)
    model_path = '../../trained_models/alexnet_cifar10_final.pth'

    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location='cpu'))
        model.eval()
        print("æ¨¡å‹åŠ è½½æˆåŠŸ!")

        # å®Œæ•´æµ‹è¯•
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)

        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

        testset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform
        )
        testloader = DataLoader(testset, batch_size=100, shuffle=False)

        accuracy = evaluate_model_full(model, testloader, device)
        print(f"å®Œæ•´æµ‹è¯•ç²¾åº¦: {accuracy:.2f}%")
    else:
        print("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")


def evaluate_model_full(model, testloader, device):
    """å®Œæ•´è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in testloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    accuracy = 100. * correct / total
    return accuracy

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='è®­ç»ƒAlexNetåœ¨CIFAR-10ä¸Š')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'], help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--resume', type=int, help='ä»å“ªä¸ªepochæ¢å¤è®­ç»ƒ')
    parser.add_argument('--total_epochs', type=int, default=80, help='æ€»è®­ç»ƒepochæ•°')

    args = parser.parse_args()

    if args.mode == 'train':
        if args.resume:
            print(f"ğŸš€ ä»epoch {args.resume}ç»§ç»­è®­ç»ƒAlexNet...")
            print(f"   ç›®æ ‡æ€»epochæ•°: {args.total_epochs}")
            print(f"   å°†ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡(0.001)ç»§ç»­ä¼˜åŒ–")
        else:
            print("ğŸš€ å¼€å§‹è®­ç»ƒAlexNet...")
            print(f"   ç›®æ ‡æ€»epochæ•°: {args.total_epochs}")
        print("-" * 50)

        model, train_losses, train_accuracies, test_accuracies = train_alexnet_cifar10(args.resume, args.total_epochs)
    else:
        test_trained_model()